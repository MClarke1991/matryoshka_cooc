{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from matryoshka_cooc.subgraph_analysis import (\n",
    "    analyze_subgraph,\n",
    "    load_matryoshka_sae,\n",
    "    load_node_info,\n",
    "    plot_pca_by_feature_activation,\n",
    "    plot_token_pca,\n",
    "    set_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "root = Path.cwd().parent.parent\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = {\n",
    "    \"sae_path\": \"checkpoints/gpt2-small_blocks.8.hook_resid_pre_24576_global-matryoshka-topk_32_0.0003_final.pt\",\n",
    "    \"cooc_dir\": \"graph_cooc_n_batches_500_layer_8/matryoshka\",\n",
    "    \"n_batches\": 10,\n",
    "    \"threshold\": 8.0,\n",
    "    \"subgraph_id\": 516,\n",
    "    \"layer\": 8,\n",
    "    \"remove_special_tokens\": True,\n",
    "    \"max_examples\": 5000,\n",
    "    \"output_dir\": \"matryoshka_pca_results\",\n",
    "}\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Run analysis\n",
    "analysis_results = analyze_subgraph(\n",
    "    sae_path=root / config[\"sae_path\"],\n",
    "    cooc_dir=root / config[\"cooc_dir\"],\n",
    "    subgraph_id=config[\"subgraph_id\"],\n",
    "    layer=config[\"layer\"],\n",
    "    activation_threshold=config[\"threshold\"],\n",
    "    n_batches=config[\"n_batches\"],\n",
    "    remove_special_tokens=config[\"remove_special_tokens\"],\n",
    "    max_examples=config[\"max_examples\"],\n",
    "    output_dir=config[\"output_dir\"],\n",
    ")\n",
    "\n",
    "# Extract components for later use\n",
    "results = analysis_results[\"results\"]\n",
    "pca_df = analysis_results[\"pca_df\"]\n",
    "pca = analysis_results[\"pca\"]\n",
    "# model = analysis_results[\"model\"]\n",
    "# subgraph_nodes = analysis_results[\"subgraph_nodes\"]\n",
    "\n",
    "# Store important data in dictionary for easy access\n",
    "analysis_data = {\n",
    "    \"pca_df\": pca_df,\n",
    "    \"tokens\": results[\"all_fired_tokens\"],\n",
    "    \"contexts\": results[\"all_token_dfs\"].context.values,\n",
    "    \"reconstructions\": results[\"all_reconstructions\"],\n",
    "    \"feature_acts\": results[\"all_graph_feature_acts\"],\n",
    "    \"max_feature_info\": results[\"all_max_feature_info\"],\n",
    "    \"examples_found\": results[\"all_examples_found\"],\n",
    "    # \"subgraph_nodes\": subgraph_nodes,\n",
    "    \"pca_explained_variance\": pca.explained_variance_ratio_\n",
    "    if pca is not None\n",
    "    else None,\n",
    "}\n",
    "\n",
    "print(f\"Analysis complete! Found {analysis_data['examples_found']} examples\")\n",
    "if pca is not None:\n",
    "    print(\"\\nPCA Explained Variance Ratios:\")\n",
    "    for i, ratio in enumerate(analysis_data[\"pca_explained_variance\"]):\n",
    "        print(f\"PC{i + 1}: {ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_token_df = analysis_results[\"results\"].all_token_dfs\n",
    "all_token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_graph_feature_acts = analysis_results[\"results\"].all_graph_feature_acts\n",
    "all_graph_feature_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pca_results = analysis_results[\"pca_df\"]\n",
    "pca_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Calculate sentiment scores for context\n",
    "def get_sentiment_scores(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "\n",
    "# Calculate sentiment scores for all contexts\n",
    "sentiment_scores = all_token_df[\"context\"].apply(get_sentiment_scores)\n",
    "\n",
    "# Convert the tensor to numpy for easier correlation analysis\n",
    "graph_features_np = all_graph_feature_acts.cpu().numpy()\n",
    "\n",
    "# Create a DataFrame with the graph features\n",
    "feature_names = [f\"feature_{i}\" for i in range(graph_features_np.shape[1])]\n",
    "graph_features_df = pd.DataFrame(graph_features_np, columns=feature_names)\n",
    "\n",
    "# Add sentiment scores to the features DataFrame\n",
    "graph_features_df[\"context_sentiment\"] = sentiment_scores.values\n",
    "\n",
    "# Calculate correlations\n",
    "correlation_df = graph_features_df.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    correlation_df[[\"context_sentiment\"]].sort_values(\n",
    "        \"context_sentiment\", ascending=False\n",
    "    )[1:6],\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    ")\n",
    "plt.title(\"Correlation between Graph Features and Context Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top 5 most correlated features\n",
    "print(\"\\nTop 5 features most correlated with sentiment:\")\n",
    "correlations = correlation_df[\"context_sentiment\"].sort_values(ascending=False)\n",
    "print(correlations[1:6])  # Skip first as it's the sentiment correlation with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "all_token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Create feature pairs and calculate correlations\n",
    "n_features = graph_features_np.shape[1]\n",
    "interaction_matrix = np.zeros((n_features, n_features))\n",
    "pair_correlations = []\n",
    "\n",
    "for i, j in combinations(range(n_features), 2):\n",
    "    interaction_term = graph_features_np[:, i] * graph_features_np[:, j]\n",
    "    correlation = np.corrcoef(interaction_term, sentiment_scores)[0, 1]\n",
    "\n",
    "    # Store in matrix for heatmap\n",
    "    interaction_matrix[i, j] = correlation\n",
    "    interaction_matrix[j, i] = correlation\n",
    "\n",
    "    # Store in list for ranking\n",
    "    pair_correlations.append((f\"feature_{i}*feature_{j}\", correlation))\n",
    "\n",
    "# Sort pairs by absolute correlation\n",
    "pair_correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Plot top 15 feature pairs\n",
    "plt.figure(figsize=(12, 6))\n",
    "pairs, corrs = zip(*pair_correlations[:15])\n",
    "plt.bar(range(len(corrs)), [abs(c) for c in corrs])\n",
    "plt.xticks(range(len(corrs)), pairs, rotation=45, ha=\"right\")\n",
    "plt.title(\"Top 15 Feature Pair Correlations with Sentiment (Absolute Value)\")\n",
    "plt.ylabel(\"|Correlation|\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print actual correlation values (including sign)\n",
    "print(\"\\nTop 15 feature pairs and their correlations with sentiment:\")\n",
    "for pair, corr in pair_correlations[:15]:\n",
    "    print(f\"{pair}: {corr:.3f}\")\n",
    "\n",
    "# Plot interaction matrix heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    interaction_matrix,\n",
    "    xticklabels=[f\"F{i}\" for i in range(n_features)],\n",
    "    yticklabels=[f\"F{i}\" for i in range(n_features)],\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    annot=False,\n",
    ")\n",
    "plt.title(\"Feature Pair Correlation Strength with Sentiment\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Create a 2x2 subplot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 15))\n",
    "fig.suptitle(\"PCA Component 2 vs 3 with Different Colorings\", fontsize=16)\n",
    "\n",
    "# Convert PCA results to numpy if needed\n",
    "if isinstance(pca_results, pd.DataFrame):\n",
    "    pca_array = pca_results.values\n",
    "else:\n",
    "    pca_array = pca_results\n",
    "\n",
    "# a) Color by sentiment\n",
    "sc0 = axs[0, 0].scatter(\n",
    "    pca_array[:, 1], pca_array[:, 2], c=sentiment_scores, cmap=\"RdYlBu\"\n",
    ")\n",
    "axs[0, 0].set_title(\"Colored by Sentiment\")\n",
    "axs[0, 0].set_xlabel(\"PC2\")\n",
    "axs[0, 0].set_ylabel(\"PC3\")\n",
    "divider = make_axes_locatable(axs[0, 0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(sc0, cax=cax)\n",
    "\n",
    "# b) Color by feature 0 activation\n",
    "sc1 = axs[0, 1].scatter(\n",
    "    pca_array[:, 1], pca_array[:, 2], c=graph_features_np[:, 0], cmap=\"viridis\"\n",
    ")\n",
    "axs[0, 1].set_title(\"Colored by Feature 0 Activation\")\n",
    "axs[0, 1].set_xlabel(\"PC2\")\n",
    "axs[0, 1].set_ylabel(\"PC3\")\n",
    "divider = make_axes_locatable(axs[0, 1])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(sc1, cax=cax)\n",
    "\n",
    "# c) Color by feature 1 activation\n",
    "sc2 = axs[1, 0].scatter(\n",
    "    pca_array[:, 1], pca_array[:, 2], c=graph_features_np[:, 1], cmap=\"viridis\"\n",
    ")\n",
    "axs[1, 0].set_title(\"Colored by Feature 1 Activation\")\n",
    "axs[1, 0].set_xlabel(\"PC2\")\n",
    "axs[1, 0].set_ylabel(\"PC3\")\n",
    "divider = make_axes_locatable(axs[1, 0])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(sc2, cax=cax)\n",
    "\n",
    "# d) Color by product of features 1 and 3\n",
    "feature_product = graph_features_np[:, 1] * graph_features_np[:, 3]\n",
    "sc3 = axs[1, 1].scatter(\n",
    "    pca_array[:, 1], pca_array[:, 2], c=feature_product, cmap=\"viridis\"\n",
    ")\n",
    "axs[1, 1].set_title(\"Colored by Feature 1 × Feature 3\")\n",
    "axs[1, 1].set_xlabel(\"PC2\")\n",
    "axs[1, 1].set_ylabel(\"PC3\")\n",
    "divider = make_axes_locatable(axs[1, 1])\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(sc3, cax=cax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlations for reference\n",
    "print(\"Correlations with sentiment:\")   \n",
    "print(f\"Feature 0: {np.corrcoef(graph_features_np[:, 0], sentiment_scores)[0, 1]:.3f}\")\n",
    "print(f\"Feature 1: {np.corrcoef(graph_features_np[:, 1], sentiment_scores)[0, 1]:.3f}\")\n",
    "print(\n",
    "    f\"Feature 1 × Feature 3: {np.corrcoef(feature_product, sentiment_scores)[0, 1]:.3f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
